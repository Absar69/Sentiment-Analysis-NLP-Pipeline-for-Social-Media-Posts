{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "885f325b-c271-4f3c-9268-b9b3d42dfd15",
   "metadata": {},
   "source": [
    "# Title: Development of an Automatic Sentiment Analysis Tool for Urdu Text on Social Media Platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b4832c-9c1a-492b-9dea-1b57d2af9c59",
   "metadata": {},
   "source": [
    "# Phase 1: Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f55b416-665d-4aa3-aab6-d1ad98bf11c0",
   "metadata": {},
   "source": [
    "# Step 1: Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbb8b5c0-de15-4f63-a298-4727723ec0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lughaatNLP in /Applications/anaconda3/lib/python3.11/site-packages (1.0.6)\n",
      "Requirement already satisfied: python-Levenshtein in /Applications/anaconda3/lib/python3.11/site-packages (from lughaatNLP) (0.26.0)\n",
      "Requirement already satisfied: tensorflow in /Applications/anaconda3/lib/python3.11/site-packages (from lughaatNLP) (2.16.1)\n",
      "Requirement already satisfied: numpy in /Applications/anaconda3/lib/python3.11/site-packages (from lughaatNLP) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in /Applications/anaconda3/lib/python3.11/site-packages (from lughaatNLP) (1.4.2)\n",
      "Requirement already satisfied: scipy in /Applications/anaconda3/lib/python3.11/site-packages (from lughaatNLP) (1.11.4)\n",
      "Requirement already satisfied: gtts in /Applications/anaconda3/lib/python3.11/site-packages (from lughaatNLP) (2.5.3)\n",
      "Requirement already satisfied: SpeechRecognition in /Applications/anaconda3/lib/python3.11/site-packages (from lughaatNLP) (3.10.4)\n",
      "Requirement already satisfied: pydub in /Applications/anaconda3/lib/python3.11/site-packages (from lughaatNLP) (0.25.1)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /Applications/anaconda3/lib/python3.11/site-packages (from gtts->lughaatNLP) (2.31.0)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in /Applications/anaconda3/lib/python3.11/site-packages (from gtts->lughaatNLP) (8.1.7)\n",
      "Requirement already satisfied: Levenshtein==0.26.0 in /Applications/anaconda3/lib/python3.11/site-packages (from python-Levenshtein->lughaatNLP) (0.26.0)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /Applications/anaconda3/lib/python3.11/site-packages (from Levenshtein==0.26.0->python-Levenshtein->lughaatNLP) (3.10.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Applications/anaconda3/lib/python3.11/site-packages (from scikit-learn->lughaatNLP) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Applications/anaconda3/lib/python3.11/site-packages (from scikit-learn->lughaatNLP) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions in /Applications/anaconda3/lib/python3.11/site-packages (from SpeechRecognition->lughaatNLP) (4.9.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorflow->lughaatNLP) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorflow->lughaatNLP) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorflow->lughaatNLP) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorflow->lughaatNLP) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorflow->lughaatNLP) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorflow->lughaatNLP) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorflow->lughaatNLP) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorflow->lughaatNLP) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorflow->lughaatNLP) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Applications/anaconda3/lib/python3.11/site-packages (from tensorflow->lughaatNLP) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorflow->lughaatNLP) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /Applications/anaconda3/lib/python3.11/site-packages (from tensorflow->lughaatNLP) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorflow->lughaatNLP) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorflow->lughaatNLP) (2.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorflow->lughaatNLP) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorflow->lughaatNLP) (1.63.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorflow->lughaatNLP) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorflow->lughaatNLP) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorflow->lughaatNLP) (0.37.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Applications/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow->lughaatNLP) (0.41.2)\n",
      "Requirement already satisfied: rich in /Applications/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow->lughaatNLP) (13.3.5)\n",
      "Requirement already satisfied: namex in /Applications/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow->lughaatNLP) (0.0.8)\n",
      "Requirement already satisfied: optree in /Applications/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow->lughaatNLP) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Applications/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->gtts->lughaatNLP) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Applications/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->gtts->lughaatNLP) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Applications/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->gtts->lughaatNLP) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->gtts->lughaatNLP) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow->lughaatNLP) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow->lughaatNLP) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow->lughaatNLP) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Applications/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow->lughaatNLP) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /Applications/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow->lughaatNLP) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Applications/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow->lughaatNLP) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Applications/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow->lughaatNLP) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install lughaatNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "370effc1-471c-4a16-bc4b-07e2a5cc043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from LughaatNLP import LughaatNLP\n",
    "textProcessor = LughaatNLP()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db1b8ed-9854-460e-b755-0fb613e6960c",
   "metadata": {},
   "source": [
    "# Step 2: Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67d450d9-97b7-41f1-a8df-36829189a3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            urdu_text  is_sarcastic  \\\n",
      "0   ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...           1.0   \n",
      "1   Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...           1.0   \n",
      "2   Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...           0.0   \n",
      "3                                        Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜           0.0   \n",
      "4    `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...           1.0   \n",
      "5         Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ ÛÛŒÚº ğŸ’”ğŸ”¥           1.0   \n",
      "6       Ø§Ù†Ø³Ø§Úº Ú©Ùˆ ØªÚ¾Ú©Ø§ Ø¯ÛŒØªØ§ ÛÛ’ Ø³ÙˆÚ†ÙˆÚº Ú©Ø§ Ø³ÙØ± Ø¨Ú¾ÛŒ ... ğŸğŸ¥€           0.0   \n",
      "7                               Ø­Ø§Ù…Ø¯ Ù…ÛŒØ± ØµØ§Ø­Ø¨ ÙˆÛŒÙ„ÚˆÙ†ğŸ‘ğŸ˜Š           0.0   \n",
      "8   ÛŒØ§Ø± ÙˆÚ†Ø§Ø±Û ÙˆÛŒÙ„Ø§ ÛÙˆÙ†Ø¯Ø§ ÛÛ’ Ø§Ø³ Ø¢Ø±Û’ Ù„Ú¯Ø§ ÛÙˆÛŒØ§ ÛÛ’ğŸ˜‚ğŸ˜‚ Øª...           1.0   \n",
      "9            ÛŒÛ Ø³Ù…Ø¬Ú¾ØªÛ’ ÛÛŒÚº Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’ ğŸ˜‚ğŸ˜‚ğŸ˜‚           1.0   \n",
      "10                      ØªØ³ÛŒ Ù„Ú‘Ø§ÚºÙ”ÛŒ Ú©Ø±ÙˆØ§Ù†ÛŒ Ø³Ø§ÚˆÛŒ Ú©ÛŒ ğŸ˜‚ğŸ˜‚ğŸ˜‚           0.0   \n",
      "11                       Ù¾Ø§Ø¦Ù† Ø¯ÙˆØ¨Ø§Ø±Û ÙØ§Ù„Ùˆ Ú©Ø±Ø¦ÛŒÛ’..ØŸØŸğŸ˜†ğŸ™„           0.0   \n",
      "12  Ú©ØªÙ†ÛŒ Ù…ÛÙ†Ú¯Ø§Ø¦ÛŒ ÛÛ’ Ø§Ù„Ùˆ Ø¯ÙˆØ³Ùˆ Ø±ÙˆÙ¾Û’ Ø¯Ø±Ø¬Ù† Ú©Ø¯Ùˆ 80Ø±ÙˆÙ¾Û’ ...           1.0   \n",
      "13   ğŸ˜Ø¹Ø´Ù‚ Ø¬Ø¨ ØªÙ… Ú©Ùˆ Ø±Ø§Ø³ Ø¢Û“ Ú¯Ø§ ğŸ’”Ø²Ø®Ù… Ú©Ú¾Ø§Ù¶ Ú¯Û’ ğŸ˜ŠÙ…ÙØ³Ú©Ø±Ø§Ù¶ Ú¯Û’           1.0   \n",
      "14                                Ú†ÙˆÙ†Ø§ Ø§ÛŒØ³Ø§ ÛÛŒ ÛÙˆØªØ§ ğŸ˜‚           0.0   \n",
      "15  Ø®Ø§ØªÙ…_Ø§Ù„Ù†Ø¨ÛŒÛŒÙ†_Ù…Ø­Ù…Ø¯ï·º Surat 73 Ø³ÙˆØ±Ø© Ø§Ù„Ù…Ø²Ù…Ù„ Ayt 20...           0.0   \n",
      "16  Ø§Ø¨ Ø¨Ø³ Ø¨Ú¾ÛŒ Ú©Ø±Ùˆ Ø¨ÛŒÚ†Ø§Ø±Û’ Ú©ÛŒ Ù¾ÛÙ„Û’ ÛÛŒ Ø¯Ùˆ Ø¨ÛŒÙˆÛŒØ§Úº ÛÛŒÛ’ ...           1.0   \n",
      "17  Ù¾ØªÛ Ù†ÛÛŒÚº Ú©ÛŒØ§ ÛÙˆØ±ÛØ§ ÛÛ’ Ø³ÛŒ Ø³ÛŒ Ú©ÛŒ Ø¨ÙˆØ±Úˆ Ú©Ùˆ Ø²Ù†Ú¯ Ù„Ú¯ ...           1.0   \n",
      "18  Ø§Ù„Ù„Û Ø¢Ù¾ Ú©Ùˆ Ø§Ù¾Ù†ÛŒ Ø±Ø­Ù…ØªÙˆÚº Ú©Û’ Ø³Ø§Ø¦Û’ Ù…ÛŒÚº Ø±Ú©Ú¾Û’ Ø§ÙˆØ± Ù…Ú©...           0.0   \n",
      "19  Ø¢Ø±Ù…ÛŒ Ú†ÛŒÙ Ú©Ø§ Ù†ÙˆÙ¹Ø³Û” Ø§Ø¨ Ù…Ø¬Ø±Ù… Ø§Ù¾Ù†Û’ Ø¬Ø±Ø§Ø¦ÛŒÙ… Ù¾Ø± Ø®ÙˆØ¯ Ù†...           0.0   \n",
      "\n",
      "    Unnamed: 2  Unnamed: 3  Unnamed: 4  Unnamed: 5 Unnamed: 6  Unnamed: 7  \n",
      "0          NaN         NaN         NaN         NaN        NaN         NaN  \n",
      "1          NaN         NaN         NaN         NaN        NaN         NaN  \n",
      "2          NaN         NaN         NaN         NaN        NaN         NaN  \n",
      "3          NaN         NaN         NaN         NaN        NaN         NaN  \n",
      "4          NaN         NaN         NaN         NaN        NaN         NaN  \n",
      "5          NaN         NaN         NaN         NaN        NaN         NaN  \n",
      "6          NaN         NaN         NaN         NaN        NaN         NaN  \n",
      "7          NaN         NaN         NaN         NaN        NaN         NaN  \n",
      "8          NaN         NaN         NaN         NaN        NaN         NaN  \n",
      "9          NaN         NaN         NaN         NaN        NaN         NaN  \n",
      "10         NaN         NaN         NaN         NaN        NaN         NaN  \n",
      "11         NaN         NaN         NaN         NaN        NaN         NaN  \n",
      "12         NaN         NaN         NaN         NaN        NaN         NaN  \n",
      "13         NaN         NaN         NaN         NaN        NaN         NaN  \n",
      "14         NaN         NaN         NaN         NaN        NaN         NaN  \n",
      "15         NaN         NaN         NaN         NaN        NaN         NaN  \n",
      "16         NaN         NaN         NaN         NaN        NaN         NaN  \n",
      "17         NaN         NaN         NaN         NaN        NaN         NaN  \n",
      "18         NaN         NaN         NaN         NaN        NaN         NaN  \n",
      "19         NaN         NaN         NaN         NaN        NaN         NaN  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data_frame = pd.read_csv('urdu_sarcastic_dataset.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data_frame.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2233d84-ef4a-46b7-b9f3-a8cd91235bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /Applications/anaconda3/lib/python3.11/site-packages (2.13.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install emoji\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dbf726-09c2-44fe-85e1-d2100439b716",
   "metadata": {},
   "source": [
    "# Step 3: Stopwords removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2703ade8-c66b-4b2e-93eb-e0e1e89ad1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Data After Stopword Removal:\n",
      "                                           urdu_text  \\\n",
      "0  ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...   \n",
      "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...   \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...   \n",
      "3                                       Ù†ÛÛŒÚº Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜   \n",
      "4   `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...   \n",
      "\n",
      "                                               clean  \n",
      "0                 ğŸ¤£ğŸ˜‚ğŸ˜‚ Ù„ÛŒÙ†Û’ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© Ú©ÙˆØ¬ÛŒ ğŸ˜ğŸ˜ğŸ˜ğŸ¤£  \n",
      "1   Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚºğŸ˜‚ğŸ˜‚  \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´Ù†...  \n",
      "3                                            Ù¾Ø§Ø¦ÛŒÙ† ğŸ˜  \n",
      "4       `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø¢Ø¦ÛŒ '' Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±ğŸ˜  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentiment_carrying_stopwords = [\"Ù†ÛÛŒÚº\", \"Ø¨Ø±Ø§\", \"ÙˆØ§Û\", \"Ø¨ÛØª\"]\n",
    "\n",
    "\n",
    "def remove_stopwords_with_lughaat(text, sentiment_stopwords):\n",
    "\n",
    "    filtered_text = textProcessor.remove_stopwords(text)\n",
    " \n",
    "    filtered_text = \" \".join([word for word in filtered_text.split() if word not in sentiment_stopwords])\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "data_frame['clean'] = data_frame['urdu_text'].apply(\n",
    "    lambda x: remove_stopwords_with_lughaat(x, sentiment_carrying_stopwords) if pd.notnull(x) else x\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nSample Data After Stopword Removal:\")\n",
    "print(data_frame[['urdu_text', 'clean']].head()\n",
    "     \n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31801f82-bb1d-467f-b974-9577c0a6ede3",
   "metadata": {},
   "source": [
    "# Step 4: Text Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee27092c-7150-4771-8c45-61ba6479b7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     positivepositivepositive Ù„ÛŒÙ†Û’ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ...\n",
      "1     Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº...\n",
      "2     Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´Ù†...\n",
      "3                                        Ù¾Ø§Ø¦ÛŒÙ† positive\n",
      "4        Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø¢Ø¦ÛŒ  Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±positive\n",
      "5              Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± negativepositive\n",
      "6                              Ø§Ù†Ø³Ø§Úº ØªÚ¾Ú©Ø§ Ø³ÙˆÚ†ÙˆÚº Ø³ÙØ±    \n",
      "7                                Ø­Ø§Ù…Ø¯ Ù…ÛŒØ± ÙˆÛŒÙ„ÚˆÙ†positive\n",
      "8     ÛŒØ§Ø± ÙˆÚ†Ø§Ø±Û ÙˆÛŒÙ„Ø§ ÛÙˆÙ†Ø¯Ø§ Ø¢Ø±Û’ ÛÙˆÛŒØ§ ÛÛ’positivepositi...\n",
      "9     Ø³Ù…Ø¬Ú¾ØªÛ’ Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’ positivepositive...\n",
      "10      ØªØ³ÛŒ Ù„Ú‘Ø§ÚºÙ”ÛŒ Ú©Ø±ÙˆØ§Ù†ÛŒ Ø³Ø§ÚˆÛŒ positivepositivepositive\n",
      "11                        Ù¾Ø§Ø¦Ù† Ø¯ÙˆØ¨Ø§Ø±Û ÙØ§Ù„Ùˆ Ú©Ø±Ø¦ÛŒÛ’   ØŸ ØŸ \n",
      "12    Ú©ØªÙ†ÛŒ Ù…ÛÙ†Ú¯Ø§Ø¦ÛŒ Ø§Ù„Ùˆ Ø¯ÙˆØ³Ùˆ Ø±ÙˆÙ¾Û’ Ø¯Ø±Ø¬Ù† Ú©Ø¯Ùˆ 80Ø±ÙˆÙ¾Û’ Ú¯Ø² ...\n",
      "13    Ø¹Ø´Ù‚ Ø±Ø§Ø³ Ø¢Û“ negativeØ²Ø®Ù… Ú©Ú¾Ø§Ù¶ Ú¯Û’ positiveÙ…ÙØ³Ú©Ø±Ø§Ù¶ Ú¯Û’\n",
      "14                                        Ú†ÙˆÙ†Ø§ positive\n",
      "15    Ø®Ø§ØªÙ…  Ø§Ù„Ù†Ø¨ÛŒÛŒÙ†  Ù…Ø­Ù…Ø¯ï·º Surat 73 Ø³ÙˆØ±Ø© Ø§Ù„Ù…Ø²Ù…Ù„ Ayt ...\n",
      "16                Ø¨ÛŒÚ†Ø§Ø±Û’ Ø¨ÛŒÙˆÛŒØ§Úº ÛÛŒÛ’ ØªÛŒØ³Ø±ÛŒ Ù¹Ø±Ø§Ø¦ positive\n",
      "17                Ù¾ØªÛ ÛÙˆØ±ÛØ§ Ø¨ÙˆØ±Úˆ Ø²Ù†Ú¯ ÛÛ’negativenegative\n",
      "18                Ø§Ù„Ù„Û Ø±Ø­Ù…ØªÙˆÚº Ø³Ø§Ø¦Û’ Ù…Ú©Ù…Ù„ ØµØ­Øª ÛŒØ§Ø¨ ÙØ±Ù…Ø§Ø¦Û’ \n",
      "19            Ø¢Ø±Ù…ÛŒ Ú†ÛŒÙ Ù†ÙˆÙ¹Ø³ Û” Ù…Ø¬Ø±Ù… Ø¬Ø±Ø§Ø¦ÛŒÙ… Ù†ÙˆÙ¹Ø³ Ú©Ø±Û’Ú¯Ø§ Û” \n",
      "20    Ú©Ú¾ÙˆØªÛŒ Ø¨Ú†ÛŒØ§ØŒØªÛŒØ±ÛŒ Ø¹Ø²Øª Ø§Û’ØŒØªÛŒØ±ÛŒ Ù‚Ø¯Ø± Ø§Û’ Û” Ø´Ú©Ø± Ø¨ÙˆÙ†Úˆ ...\n",
      "21    ÙÙˆØ¬ Ø³ÛŒØ§Ø³Øª Ø¯ÙˆØ± Û” Ø¨Ù„Ø§ÙˆÙ„ Ø²Ø±Ø¯Ø§Ø±ÛŒ ÙÙˆØ¬ Ú©Ø±Ø§Ú†ÛŒ ÙˆØ§Ù‚Ø¹Û’ Ù†...\n",
      "22                                   Ù„Ú‘Ú©ÛŒ Ø¬ÛŒØªÛŒ positive\n",
      "23    ØµØ¯Ø§Ø¦ÛŒÚº Ø¯Ø±ÙˆØ¯ÙˆÚº Ø±ÛÛŒÙ†Ú¯ÛŒ Ø³Ù† Ø¯Ù„ Ø´Ø§Ø¯ Ú¯Ø§ØŒ Ø®Ø¯Ø§ Ø§ÛÙ„Ø³Ù†Øª ...\n",
      "24                                ÙˆÙˆÙ¹ Ø¨Ø¬Ø§Ø¦Û’ ÙˆÙˆÙ¹Ø± Ø¹Ø²Ù‘Øª ï¸\n",
      "25    2004 Ù†Ø´Ø¦ÛŒ Ú©ÛØ§      Ø§Ø³Ù¹ÛŒØ¨Ù„Ø´Ù…Ù†Ù¹ Ú©ØªØ§ ÙˆØ²ÛŒØ±Ø§Ø¹Ø¸Ù… 201...\n",
      "26                       Ù…Ø´Ú©Ù„ Ù†Ø§Ù…Ù…Ú©Ù† Ú©Ø§Ù…Ø±Ø§Ù† Ú©ÙˆØ´Ø´ Û” Û” ØŸ \n",
      "27          Ù¾ÙˆØ±Û’ ÛÙØªÛ’ Ø³Ø§Ù„Ù† Ø±ÙˆÙ¹ÛŒ Ú©Ú¾Ø§ÛŒØ§ Ú©ÙˆØ¦ÛŒ Ø­Ø§Ù„ negative\n",
      "28                                     positivepositive\n",
      "29    ÙˆÚ¾ Ú©ÙˆØ¦ÛŒ Ú¾Ùˆ Ø¨ÛŒØ´Ú© Ú¾Û’ Ú¾Ùˆ  Ú©ÛŒÙ…Ù†Ø§ Ø­Ø±Ø§Ù… Ú¾Ùˆ Ø­Ù‚ Ø¨Ø§Øª Ú©Ø§...\n",
      "30    Ù…Ø§Ø¶ÛŒ Ø§Ù†ØªÛØ§Ø¦ÛŒ Ø¨Û’ ÙˆÙ‚ÙˆÙ Ø­Ø§Ù„ Ø³Ù…Ø¬Ú¾Ø¯Ø§Ø± Ù„Ú‘Ú©ÛŒ ÛÙˆÚº Ù…Ø±ÛŒÙ…...\n",
      "31                                       Ø¢Ù…ÛŒÙ† Ø«Ù…Û Ø¢Ù…ÛŒÙ† \n",
      "32    Ø¬ÛŒØªÚ¾Û’ Ú©Ú¾ÙˆØªÛŒ ÙˆØªÚ¾Û’ Ø¢ Ú©Ú¾Ù„ÙˆØªÛŒ positivepositiveposi...\n",
      "33    Ø§Ø³Ù…Ø¨Ù„ÛŒ ÚˆÛŒØ²Ù„ Ú†Ù„ Ø§ÙØ³Ú©Ùˆ Ú¯Ø¯Ú¾Ø§  Ú¾Û’ Ú¯Ø¯Ú¾Û’ ÚˆÛŒØ²Ù„  ÚˆÙ†ÚˆÛ’ ...\n",
      "34    Ø¹Ø¬ÛŒØ¨ Ø¨Ø§Øª Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Û” Ù¾Ù†Ú¯Ø§ Ú¯ÙˆÙ„ÛŒØ§Úº Ù…Ø­Ùˆ Ø±Ù‚Øµ Û” Ù¾Øª...\n",
      "35                                     Ú¯Úˆ Ù†Ø§Ù¸ÛŒÙ¹positive\n",
      "36    Ø¯Ø±ÙˆØ§Ø²Û’ Ø§ØªØ§Ø±ØªÛ’ ØªÚ¾Ù¾Ú‘ Ù„Ú¯Ø§ØªÛ’ positivepositiveposit...\n",
      "37                      Ú©Ù„ÛØ§Ú‘ÛŒ Ù…Ø§Ø±Ù†Ø§ Ú©ÙˆØ¦ Ø³ÛŒÚ©Ú¾Û’ positive\n",
      "38          Ø¯Ù„ÛÙ† Ù¾Ú¾Ù¾Ú¾Ùˆ positivepositivepositivepositive\n",
      "39    ÛØ§ÛØ§ÛØ§ÛØ§ÛØ§ÛØ§ÛØ§ÛØ§ÛØ§ÛØ§ÛØ§ÛØ§ÛØ§ÛØ§ÛØ§ÛØ§ Ú©Ù…ÛŒÙ†Û positiv...\n",
      "40    Ù…Ø­Ù…Ø¯ Ù†Ø¯ÛŒÙ… Ù…Ù„Ú© Ø¨Ø·ÙˆØ± Ø¹Ø§Ù… Ù¾Ø§Ú©Ø³ØªØ§Ù†ÛŒ Ø¨Ù„Ø§ÙˆÙ„ Ø²Ø±Ø¯Ø§Ø±ÛŒ Ø¯...\n",
      "41    Ù‚Ø·Ø±ÛŒ Ù†Ø§Ù†ÛŒ Û” Û” Ø¨Ù„Ùˆ Ø±Ø§Ù†ÛŒ Ø°Ù„ÛŒÙ„ ÙˆÙ‚Øª Ú†Ø§ÛØªØ§ Û” Û” Û” Û” ...\n",
      "42    Ø§Ú†Ú¾Ø§ Ø¨Ú¾Ø§Ø¦ÛŒÙˆÚº ÙˆÛŒÙ„Ú©Ù… Ù„Ú‘Ú©ÛŒ Ø¨ÛÙ†ÙˆÚº Ú©Ø±Ø§ Ù†ÛÛŒÚºpositive...\n",
      "43              Ù…ÙˆÙ¹ÛŒ Ú†Ú‘ÛŒÙ„ Ù¹ÙˆÛŒÙ¹ positivepositivepositive\n",
      "44           Ø§ÛŒØªÚ¾Û’ ØªÛ’ Ø¬Ø¬ Û” Û” Û” positivepositivepositive\n",
      "45    Ø³Ø§Ø±Ø§ Ø¯Ù† Ù¾Ø§Ú© ÙÙˆØ¬ Ø®Ù„Ø§Ù Ø¨Ú©ÙˆØ§Ø³ Ø±Ø§Øª ÙÙˆØ¬ÛŒ Ú¯Ù„ Û” Û” Û” p...\n",
      "46    Ù„Ú¯ØªØ§ Ø¯ÛŒÙˆØªØ§ Ù¾ÙˆØ±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù†  Ù„Ú©ÛŒØ±ÙˆÚº  Ø³ÛØ§Ø±Û’ Ø³ÙˆÚ†ØªØ§ Ø¨...\n",
      "47    ÛØ§ÛØ§ÛØ§ÛØ§ÛØ§ÛØ§ÛØ§ÛØ§ÛØ§ÛØ§ÛØ§Ø§ÛØ§ÛØ§ÛØ§ÛØ§ positivepositi...\n",
      "48    Ø¶Ø±ÙˆØ±Øª Ø§Ù„Ù¹ÛŒ Ú©Ú¾ÙˆÙ¾Ú‘ÛŒ Ø¨Ø§Øª Ú©Ø§Ù† Ø³Ù†Ùˆ Ù†Ú©Ø§Ù„ positivepos...\n",
      "49    Ø³Ù†Ø§ 1 Ø§Ú©ÛŒÙ„Ø§ 1 11 Ú¯ÛŒØ§Ø±Û Û” ØªØ§Ø±ÛŒØ® Ø¯ÙØ¹Û 11 1 Ø¨Ø±Ø¨Ø§Ø¯...\n",
      "Name: cleaned_text1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import emoji\n",
    "\n",
    "emoji_sentiment_dict = {\n",
    "    \"ğŸ˜Š\": \"positive\", \"ğŸ˜¢\": \"negative\", \"ğŸ˜‚\": \"positive\", \"ğŸ¤£\": \"positive\",\n",
    "    \"ğŸ˜\": \"positive\", \"ğŸ˜¡\": \"negative\", \"â¤ï¸\": \"positive\", \"ğŸ’”\": \"negative\",\n",
    "    \"ğŸ‘\": \"positive\", \"ğŸ‘\": \"negative\", \"ğŸ˜­\": \"negative\", \"ğŸ˜œ\": \"positive\",\n",
    "    \"ğŸ˜\": \"positive\", \"ğŸ”¥\": \"positive\", \"ğŸ˜\": \"negative\"\n",
    "}\n",
    "\n",
    "\n",
    "def clean_text_with_lughaat(text, emoji_dict):\n",
    "    if not isinstance(text, str):  \n",
    "        return ''  \n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    # Remove hashtags (optional)\n",
    "    text = re.sub(r\"#\\w+\", '', text)\n",
    "    # Remove punctuations (except those in the sentiment dict)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Replace recognized emojis with their sentiment labels\n",
    "    cleaned_text = []\n",
    "    for char in text:\n",
    "        if char in emoji_sentiment_dict:\n",
    "            cleaned_text.append(emoji_sentiment_dict[char])  # Replace with sentiment label\n",
    "        elif not emoji.is_emoji(char):  # Check if it's not an emoji\n",
    "            cleaned_text.append(char)  # Keep original character\n",
    "\n",
    "    return ''.join(cleaned_text)  \n",
    "# Apply cleaning function\n",
    "data_frame['cleaned_text1'] = data_frame['clean'].apply(\n",
    "    lambda x: clean_text_with_lughaat(str(x), emoji_sentiment_dict) if pd.notnull(x) else x\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(data_frame['cleaned_text1'].head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f40cd7",
   "metadata": {},
   "source": [
    "# Step 5: Applying the filtering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5df091a9-9da9-49d8-9ec9-b203ec03be92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text1  \\\n",
      "0   positivepositivepositive Ù„ÛŒÙ†Û’ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ...   \n",
      "1   Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº...   \n",
      "2   Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´Ù†...   \n",
      "4      Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø¢Ø¦ÛŒ  Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±positive   \n",
      "5            Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± negativepositive   \n",
      "6                            Ø§Ù†Ø³Ø§Úº ØªÚ¾Ú©Ø§ Ø³ÙˆÚ†ÙˆÚº Ø³ÙØ±       \n",
      "7                              Ø­Ø§Ù…Ø¯ Ù…ÛŒØ± ÙˆÛŒÙ„ÚˆÙ†positive   \n",
      "8   ÛŒØ§Ø± ÙˆÚ†Ø§Ø±Û ÙˆÛŒÙ„Ø§ ÛÙˆÙ†Ø¯Ø§ Ø¢Ø±Û’ ÛÙˆÛŒØ§ ÛÛ’positivepositi...   \n",
      "9   Ø³Ù…Ø¬Ú¾ØªÛ’ Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’ positivepositive...   \n",
      "10    ØªØ³ÛŒ Ù„Ú‘Ø§ÚºÙ”ÛŒ Ú©Ø±ÙˆØ§Ù†ÛŒ Ø³Ø§ÚˆÛŒ positivepositivepositive   \n",
      "11                      Ù¾Ø§Ø¦Ù† Ø¯ÙˆØ¨Ø§Ø±Û ÙØ§Ù„Ùˆ Ú©Ø±Ø¦ÛŒÛ’   ØŸ ØŸ    \n",
      "12  Ú©ØªÙ†ÛŒ Ù…ÛÙ†Ú¯Ø§Ø¦ÛŒ Ø§Ù„Ùˆ Ø¯ÙˆØ³Ùˆ Ø±ÙˆÙ¾Û’ Ø¯Ø±Ø¬Ù† Ú©Ø¯Ùˆ 80Ø±ÙˆÙ¾Û’ Ú¯Ø² ...   \n",
      "13  Ø¹Ø´Ù‚ Ø±Ø§Ø³ Ø¢Û“ negativeØ²Ø®Ù… Ú©Ú¾Ø§Ù¶ Ú¯Û’ positiveÙ…ÙØ³Ú©Ø±Ø§Ù¶ Ú¯Û’   \n",
      "15  Ø®Ø§ØªÙ…  Ø§Ù„Ù†Ø¨ÛŒÛŒÙ†  Ù…Ø­Ù…Ø¯ï·º Surat 73 Ø³ÙˆØ±Ø© Ø§Ù„Ù…Ø²Ù…Ù„ Ayt ...   \n",
      "16              Ø¨ÛŒÚ†Ø§Ø±Û’ Ø¨ÛŒÙˆÛŒØ§Úº ÛÛŒÛ’ ØªÛŒØ³Ø±ÛŒ Ù¹Ø±Ø§Ø¦ positive   \n",
      "17              Ù¾ØªÛ ÛÙˆØ±ÛØ§ Ø¨ÙˆØ±Úˆ Ø²Ù†Ú¯ ÛÛ’negativenegative   \n",
      "18              Ø§Ù„Ù„Û Ø±Ø­Ù…ØªÙˆÚº Ø³Ø§Ø¦Û’ Ù…Ú©Ù…Ù„ ØµØ­Øª ÛŒØ§Ø¨ ÙØ±Ù…Ø§Ø¦Û’    \n",
      "19          Ø¢Ø±Ù…ÛŒ Ú†ÛŒÙ Ù†ÙˆÙ¹Ø³ Û” Ù…Ø¬Ø±Ù… Ø¬Ø±Ø§Ø¦ÛŒÙ… Ù†ÙˆÙ¹Ø³ Ú©Ø±Û’Ú¯Ø§ Û”    \n",
      "20  Ú©Ú¾ÙˆØªÛŒ Ø¨Ú†ÛŒØ§ØŒØªÛŒØ±ÛŒ Ø¹Ø²Øª Ø§Û’ØŒØªÛŒØ±ÛŒ Ù‚Ø¯Ø± Ø§Û’ Û” Ø´Ú©Ø± Ø¨ÙˆÙ†Úˆ ...   \n",
      "21  ÙÙˆØ¬ Ø³ÛŒØ§Ø³Øª Ø¯ÙˆØ± Û” Ø¨Ù„Ø§ÙˆÙ„ Ø²Ø±Ø¯Ø§Ø±ÛŒ ÙÙˆØ¬ Ú©Ø±Ø§Ú†ÛŒ ÙˆØ§Ù‚Ø¹Û’ Ù†...   \n",
      "\n",
      "                                        filtered_text  \n",
      "0   positivepositivepositive Ù„ÛŒÙ†Û’ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ...  \n",
      "1   Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº...  \n",
      "2   Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´Ù†...  \n",
      "4      Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø¢Ø¦ÛŒ  Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±positive  \n",
      "5            Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± negativepositive  \n",
      "6                            Ø§Ù†Ø³Ø§Úº ØªÚ¾Ú©Ø§ Ø³ÙˆÚ†ÙˆÚº Ø³ÙØ±      \n",
      "7                              Ø­Ø§Ù…Ø¯ Ù…ÛŒØ± ÙˆÛŒÙ„ÚˆÙ†positive  \n",
      "8   ÛŒØ§Ø± ÙˆÚ†Ø§Ø±Û ÙˆÛŒÙ„Ø§ ÛÙˆÙ†Ø¯Ø§ Ø¢Ø±Û’ ÛÙˆÛŒØ§ ÛÛ’positivepositi...  \n",
      "9   Ø³Ù…Ø¬Ú¾ØªÛ’ Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’ positivepositive...  \n",
      "10    ØªØ³ÛŒ Ù„Ú‘Ø§ÚºÙ”ÛŒ Ú©Ø±ÙˆØ§Ù†ÛŒ Ø³Ø§ÚˆÛŒ positivepositivepositive  \n",
      "11                      Ù¾Ø§Ø¦Ù† Ø¯ÙˆØ¨Ø§Ø±Û ÙØ§Ù„Ùˆ Ú©Ø±Ø¦ÛŒÛ’   ØŸ ØŸ   \n",
      "12  Ú©ØªÙ†ÛŒ Ù…ÛÙ†Ú¯Ø§Ø¦ÛŒ Ø§Ù„Ùˆ Ø¯ÙˆØ³Ùˆ Ø±ÙˆÙ¾Û’ Ø¯Ø±Ø¬Ù† Ú©Ø¯Ùˆ 80Ø±ÙˆÙ¾Û’ Ú¯Ø² ...  \n",
      "13  Ø¹Ø´Ù‚ Ø±Ø§Ø³ Ø¢Û“ negativeØ²Ø®Ù… Ú©Ú¾Ø§Ù¶ Ú¯Û’ positiveÙ…ÙØ³Ú©Ø±Ø§Ù¶ Ú¯Û’  \n",
      "15  Ø®Ø§ØªÙ…  Ø§Ù„Ù†Ø¨ÛŒÛŒÙ†  Ù…Ø­Ù…Ø¯ï·º Surat 73 Ø³ÙˆØ±Ø© Ø§Ù„Ù…Ø²Ù…Ù„ Ayt ...  \n",
      "16              Ø¨ÛŒÚ†Ø§Ø±Û’ Ø¨ÛŒÙˆÛŒØ§Úº ÛÛŒÛ’ ØªÛŒØ³Ø±ÛŒ Ù¹Ø±Ø§Ø¦ positive  \n",
      "17              Ù¾ØªÛ ÛÙˆØ±ÛØ§ Ø¨ÙˆØ±Úˆ Ø²Ù†Ú¯ ÛÛ’negativenegative  \n",
      "18              Ø§Ù„Ù„Û Ø±Ø­Ù…ØªÙˆÚº Ø³Ø§Ø¦Û’ Ù…Ú©Ù…Ù„ ØµØ­Øª ÛŒØ§Ø¨ ÙØ±Ù…Ø§Ø¦Û’   \n",
      "19          Ø¢Ø±Ù…ÛŒ Ú†ÛŒÙ Ù†ÙˆÙ¹Ø³ Û” Ù…Ø¬Ø±Ù… Ø¬Ø±Ø§Ø¦ÛŒÙ… Ù†ÙˆÙ¹Ø³ Ú©Ø±Û’Ú¯Ø§ Û”   \n",
      "20  Ú©Ú¾ÙˆØªÛŒ Ø¨Ú†ÛŒØ§ØŒØªÛŒØ±ÛŒ Ø¹Ø²Øª Ø§Û’ØŒØªÛŒØ±ÛŒ Ù‚Ø¯Ø± Ø§Û’ Û” Ø´Ú©Ø± Ø¨ÙˆÙ†Úˆ ...  \n",
      "21  ÙÙˆØ¬ Ø³ÛŒØ§Ø³Øª Ø¯ÙˆØ± Û” Ø¨Ù„Ø§ÙˆÙ„ Ø²Ø±Ø¯Ø§Ø±ÛŒ ÙÙˆØ¬ Ú©Ø±Ø§Ú†ÛŒ ÙˆØ§Ù‚Ø¹Û’ Ù†...  \n"
     ]
    }
   ],
   "source": [
    "def filter_short_posts(text):\n",
    "    return len(text.split()) >= 3  # Keep posts with 3 or more words\n",
    "\n",
    "# Apply the filtering function\n",
    "data_frame['filtered_text'] = data_frame['cleaned_text1'].apply(lambda x: x if filter_short_posts(x) else None)\n",
    "\n",
    "# Drop rows with None values in 'filtered_text'\n",
    "data_frame.dropna(subset=['filtered_text'], inplace=True)\n",
    "\n",
    "# Display the filtered results\n",
    "print(data_frame[['cleaned_text1', 'filtered_text']].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de01c9",
   "metadata": {},
   "source": [
    "# Replacing 'urdu_text' column with 'filtered_text' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2a5389a5-2a2a-40bf-88e6-97e92b3e7ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Preprocessed Urdu Text:\n",
      "                                            urdu_text\n",
      "0   positivepositivepositive Ù„ÛŒÙ†Û’ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ...\n",
      "1   Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº...\n",
      "2   Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´Ù†...\n",
      "4      Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø¢Ø¦ÛŒ  Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±positive\n",
      "5            Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± negativepositive\n",
      "6                            Ø§Ù†Ø³Ø§Úº ØªÚ¾Ú©Ø§ Ø³ÙˆÚ†ÙˆÚº Ø³ÙØ±    \n",
      "7                              Ø­Ø§Ù…Ø¯ Ù…ÛŒØ± ÙˆÛŒÙ„ÚˆÙ†positive\n",
      "8   ÛŒØ§Ø± ÙˆÚ†Ø§Ø±Û ÙˆÛŒÙ„Ø§ ÛÙˆÙ†Ø¯Ø§ Ø¢Ø±Û’ ÛÙˆÛŒØ§ ÛÛ’positivepositi...\n",
      "9   Ø³Ù…Ø¬Ú¾ØªÛ’ Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û’ positivepositive...\n",
      "10    ØªØ³ÛŒ Ù„Ú‘Ø§ÚºÙ”ÛŒ Ú©Ø±ÙˆØ§Ù†ÛŒ Ø³Ø§ÚˆÛŒ positivepositivepositive\n",
      "11                      Ù¾Ø§Ø¦Ù† Ø¯ÙˆØ¨Ø§Ø±Û ÙØ§Ù„Ùˆ Ú©Ø±Ø¦ÛŒÛ’   ØŸ ØŸ \n",
      "12  Ú©ØªÙ†ÛŒ Ù…ÛÙ†Ú¯Ø§Ø¦ÛŒ Ø§Ù„Ùˆ Ø¯ÙˆØ³Ùˆ Ø±ÙˆÙ¾Û’ Ø¯Ø±Ø¬Ù† Ú©Ø¯Ùˆ 80Ø±ÙˆÙ¾Û’ Ú¯Ø² ...\n",
      "13  Ø¹Ø´Ù‚ Ø±Ø§Ø³ Ø¢Û“ negativeØ²Ø®Ù… Ú©Ú¾Ø§Ù¶ Ú¯Û’ positiveÙ…ÙØ³Ú©Ø±Ø§Ù¶ Ú¯Û’\n",
      "15  Ø®Ø§ØªÙ…  Ø§Ù„Ù†Ø¨ÛŒÛŒÙ†  Ù…Ø­Ù…Ø¯ï·º Surat 73 Ø³ÙˆØ±Ø© Ø§Ù„Ù…Ø²Ù…Ù„ Ayt ...\n",
      "16              Ø¨ÛŒÚ†Ø§Ø±Û’ Ø¨ÛŒÙˆÛŒØ§Úº ÛÛŒÛ’ ØªÛŒØ³Ø±ÛŒ Ù¹Ø±Ø§Ø¦ positive\n",
      "17              Ù¾ØªÛ ÛÙˆØ±ÛØ§ Ø¨ÙˆØ±Úˆ Ø²Ù†Ú¯ ÛÛ’negativenegative\n",
      "18              Ø§Ù„Ù„Û Ø±Ø­Ù…ØªÙˆÚº Ø³Ø§Ø¦Û’ Ù…Ú©Ù…Ù„ ØµØ­Øª ÛŒØ§Ø¨ ÙØ±Ù…Ø§Ø¦Û’ \n",
      "19          Ø¢Ø±Ù…ÛŒ Ú†ÛŒÙ Ù†ÙˆÙ¹Ø³ Û” Ù…Ø¬Ø±Ù… Ø¬Ø±Ø§Ø¦ÛŒÙ… Ù†ÙˆÙ¹Ø³ Ú©Ø±Û’Ú¯Ø§ Û” \n",
      "20  Ú©Ú¾ÙˆØªÛŒ Ø¨Ú†ÛŒØ§ØŒØªÛŒØ±ÛŒ Ø¹Ø²Øª Ø§Û’ØŒØªÛŒØ±ÛŒ Ù‚Ø¯Ø± Ø§Û’ Û” Ø´Ú©Ø± Ø¨ÙˆÙ†Úˆ ...\n",
      "21  ÙÙˆØ¬ Ø³ÛŒØ§Ø³Øª Ø¯ÙˆØ± Û” Ø¨Ù„Ø§ÙˆÙ„ Ø²Ø±Ø¯Ø§Ø±ÛŒ ÙÙˆØ¬ Ú©Ø±Ø§Ú†ÛŒ ÙˆØ§Ù‚Ø¹Û’ Ù†...\n",
      "22                                 Ù„Ú‘Ú©ÛŒ Ø¬ÛŒØªÛŒ positive\n",
      "23  ØµØ¯Ø§Ø¦ÛŒÚº Ø¯Ø±ÙˆØ¯ÙˆÚº Ø±ÛÛŒÙ†Ú¯ÛŒ Ø³Ù† Ø¯Ù„ Ø´Ø§Ø¯ Ú¯Ø§ØŒ Ø®Ø¯Ø§ Ø§ÛÙ„Ø³Ù†Øª ...\n",
      "24                              ÙˆÙˆÙ¹ Ø¨Ø¬Ø§Ø¦Û’ ÙˆÙˆÙ¹Ø± Ø¹Ø²Ù‘Øª ï¸\n",
      "25  2004 Ù†Ø´Ø¦ÛŒ Ú©ÛØ§      Ø§Ø³Ù¹ÛŒØ¨Ù„Ø´Ù…Ù†Ù¹ Ú©ØªØ§ ÙˆØ²ÛŒØ±Ø§Ø¹Ø¸Ù… 201...\n",
      "26                     Ù…Ø´Ú©Ù„ Ù†Ø§Ù…Ù…Ú©Ù† Ú©Ø§Ù…Ø±Ø§Ù† Ú©ÙˆØ´Ø´ Û” Û” ØŸ \n",
      "27        Ù¾ÙˆØ±Û’ ÛÙØªÛ’ Ø³Ø§Ù„Ù† Ø±ÙˆÙ¹ÛŒ Ú©Ú¾Ø§ÛŒØ§ Ú©ÙˆØ¦ÛŒ Ø­Ø§Ù„ negative\n",
      "29  ÙˆÚ¾ Ú©ÙˆØ¦ÛŒ Ú¾Ùˆ Ø¨ÛŒØ´Ú© Ú¾Û’ Ú¾Ùˆ  Ú©ÛŒÙ…Ù†Ø§ Ø­Ø±Ø§Ù… Ú¾Ùˆ Ø­Ù‚ Ø¨Ø§Øª Ú©Ø§...\n",
      "30  Ù…Ø§Ø¶ÛŒ Ø§Ù†ØªÛØ§Ø¦ÛŒ Ø¨Û’ ÙˆÙ‚ÙˆÙ Ø­Ø§Ù„ Ø³Ù…Ø¬Ú¾Ø¯Ø§Ø± Ù„Ú‘Ú©ÛŒ ÛÙˆÚº Ù…Ø±ÛŒÙ…...\n",
      "31                                     Ø¢Ù…ÛŒÙ† Ø«Ù…Û Ø¢Ù…ÛŒÙ† \n",
      "32  Ø¬ÛŒØªÚ¾Û’ Ú©Ú¾ÙˆØªÛŒ ÙˆØªÚ¾Û’ Ø¢ Ú©Ú¾Ù„ÙˆØªÛŒ positivepositiveposi...\n"
     ]
    }
   ],
   "source": [
    "# Replace 'urdu_text' column with 'filtered_text' column\n",
    "data_frame['urdu_text'] = data_frame['filtered_text']\n",
    "print(\"Final Preprocessed Urdu Text:\")\n",
    "print(data_frame[['urdu_text']].head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddc66df-05b8-4d02-bcc1-f74164408667",
   "metadata": {},
   "source": [
    "# Phase 2: Stemming and Lemmatization for Urdu Text using LughaatNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed886f9",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "80626c84-075c-4364-819a-f6b5ab926cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Data After Stemming:\n",
      "                                           urdu_text  \\\n",
      "0  positivepositivepositive Ù„ÛŒÙ†Û’ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ...   \n",
      "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚº...   \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´Ù†...   \n",
      "4     Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø¢Ø¦ÛŒ  Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±positive   \n",
      "5           Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± negativepositive   \n",
      "\n",
      "                                        stemmed_text  \n",
      "0  positivepositivepositive Ù„ÛŒÙ†Û Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ...  \n",
      "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†Ø§ Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†Ø§ Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚºpo...  \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´Ù†...  \n",
      "4      Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø¢Ø¦ÛŒ Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±positive  \n",
      "5             Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø± negativepositive  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def apply_stemming(text):\n",
    "    stemmed_text = textProcessor.urdu_stemmer(text)\n",
    "    return stemmed_text\n",
    "\n",
    "# Apply stemming to the 'cleaned_text' column\n",
    "data_frame['stemmed_text'] = data_frame['urdu_text'].apply(lambda x: apply_stemming(str(x)) if pd.notnull(x) else x)\n",
    "\n",
    "# Display the first few rows after stemming\n",
    "print(\"\\nSample Data After Stemming:\")\n",
    "print(data_frame[['urdu_text', 'stemmed_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7127406e-b6fc-407f-a7eb-32a58e8ca29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Preprocessed Urdu Text:\n",
      "                                            urdu_text\n",
      "0   positivepositivepositive Ù„ÛŒÙ†Û Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ...\n",
      "1   Ú†Ù„ Ù…ÛÙ…Ø§Ù†Ø§ Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†Ø§ Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚºpo...\n",
      "2   Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´Ù†...\n",
      "4       Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø¢Ø¦ÛŒ Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±positive\n",
      "5              Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø± negativepositive\n",
      "6                                 Ø§Ù†Ø³Ø§Úº ØªÚ¾Ú©Ø§ Ø³ÙˆÚ†Ø§ Ø³ÙØ±\n",
      "7                              Ø­Ø§Ù…Ø¯ Ù…ÛŒØ± ÙˆÛŒÙ„ÚˆÙ†positive\n",
      "8   ÛŒØ§Ø± ÙˆÚ†Ø§Ø±Û ÙˆÛŒÙ„Ø§ ÛÙˆÙ†Ø¯Ø§ Ø¢Ø±Û ÛÙˆÛŒØ§ ÛÛ’positivepositi...\n",
      "9   Ø³Ù…Ø¬Ú¾ØªÛ Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªÙ† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û positivepositivep...\n",
      "10    ØªØ³ÛŒ Ù„Ú‘Ø§ÚºÙ”ÛŒ Ú©Ø±ÙˆØ§Ù†ÛŒ Ø³Ø§ÚˆÛŒ positivepositivepositive\n",
      "11                          Ù¾Ø§Ø¦Ù† Ø¯ÙˆØ¨Ø±Û ÙØ§Ù„Ùˆ Ú©Ø±Ø¦ÛŒÛ ØŸ ØŸ\n",
      "12  Ú©ØªÙ†ÛŒ Ù…ÛÙ†Ú¯Ø§Ø¦ÛŒ Ø§Ù„Ùˆ Ø¯ÙˆØ³Ùˆ Ø±ÙˆÙ¾Û Ø¯Ø±Ø¬Ù† Ú©Ø¯Ùˆ 80Ø±ÙˆÙ¾Û Ú¯Ø² ...\n",
      "13  Ø¹Ø´Ù‚ Ø±Ø§Ø³ Ø¢Û“ negativeØ²Ø®Ù… Ú©Ú¾Ø§Ù¶ Ú¯Û positiveÙ…ÙØ³Ú©Ø±Ø§Ù¶ Ú¯Û\n",
      "15  Ø®Ø§ØªÙ… Ø§Ù„Ù†Ø¨ÛŒÛŒÙ† Ù…Ø­Ù…Ø¯ï·º Surat 73 Ø³ÙˆØ±Ù‡ Ø§Ù„Ù…Ø²Ù…Ù„ Ayt 20...\n",
      "16                Ø¨ÛŒÚ†Ø§Ø±Û Ø¨ÛŒÙˆÛŒ ÛÛŒÛ ØªÛŒØ³Ø±ÛŒ Ù¹Ø±Ø§Ø¦ positive\n",
      "17              Ù¾ØªÛ ÛÙˆØ±ÛØ§ Ø¨ÙˆØ±Úˆ Ø²Ù†Ú¯ ÛÛ’negativenegative\n",
      "18                 Ø§Ù„Ù„Û Ø±Ø­Ù…Øª Ø³Ø§Ø¦Û Ù…Ú©Ù…Ù„ ØµØ­Øª ÛŒØ§Ø¨ ÙØ±Ù…Ø§Ø¦Û\n",
      "19           Ø¢Ø±Ù…ÛŒ Ú†ÛŒÙ Ù†ÙˆÙ¹Ø³ Û” Ù…Ø¬Ø±Ù… Ø¬Ø±Ø§Ø¦ÛŒÙ… Ù†ÙˆÙ¹Ø³ Ú©Ø±Û’Ú¯Ø§ Û”\n",
      "20  Ú©Ú¾ÙˆØªÛŒ Ø¨Ú†ÛŒØ§ØŒØªÛŒØ±ÛŒ Ø¹Ø²Øª Ø§ÛØŒØªÛŒØ±ÛŒ Ù‚Ø¯Ø± Ø§Û Û” Ø´Ú©Ø± Ø¨ÙˆÙ†Úˆ ...\n",
      "21  ÙÙˆØ¬ Ø³ÛŒØ§Ø³Øª Ø¯ÙˆØ± Û” Ø¨Ù„Ø§ÙˆÙ„ Ø²Ø±Ø¯Ø§Ø±ÛŒ ÙÙˆØ¬ Ú©Ø±Ø§Ú†ÛŒ ÙˆØ§Ù‚Ø¹Û Ù†...\n",
      "22                                 Ù„Ú‘Ú©ÛŒ Ø¬ÛŒØªÛŒ positive\n",
      "23  ØµØ¯Ø§Ø¦Ø§ Ø¯Ø±ÙˆØ¯Ø§ Ø±ÛÛŒÙ†Ú¯ÛŒ Ø³Ù† Ø¯Ù„ Ø´Ø§Ø¯ Ú¯Ø§ØŒ Ø®Ø¯Ø§ Ø§ÛÙ„Ø³Ù†Øª Ø¢Ø¨...\n",
      "24                              ÙˆÙˆÙ¹ Ø¨Ø¬Ø§Ø¦Û ÙˆÙˆÙ¹Ø± Ø¹Ø²Ù‘Øª ï¸\n",
      "25  2004 Ù†Ø´Ø¦ÛŒ Ú©ÛØ§ Ø§Ø³Ù¹ÛŒØ¨Ù„Ø´Ù…Ù†Ù¹ Ú©Øª ÙˆØ²ÛŒØ±Ø§Ø¹Ø¸Ù… 2018 Ú©ØªÛ ...\n",
      "26                      Ù…Ø´Ú©Ù„ Ù†Ø§Ù…Ù…Ú©Ù† Ú©Ø§Ù…Ø±Ø§Ù† Ú©ÙˆØ´Ø´ Û” Û” ØŸ\n",
      "27        Ù¾ÙˆØ±Û ÛÙØªÛ Ø³Ø§Ù„Ù† Ø±ÙˆÙ¹ÛŒ Ú©Ú¾Ø§ÛŒØ§ Ú©ÙˆØ¦ÛŒ Ø­Ø§Ù„ negative\n",
      "29  ÙˆÚ¾ Ú©ÙˆØ¦ÛŒ Ú¾Ùˆ Ø¨ÛŒØ´Ú© Ú¾Û Ú¾Ùˆ Ú©ÛŒÙ…Ù†Ø§ Ø­Ø±Ø§Ù… Ú¾Ùˆ Ø­Ù‚ Ø¨Ù‡ Ú©Ø§Ù…Ø±...\n",
      "30  Ù…Ø§Ø¶ÛŒ Ø§Ù†ØªÛØ§Ø¦ÛŒ Ø¨Û ÙˆÙ‚ÙˆÙ Ø­Ø§Ù„ Ø³Ù…Ø¬Ú¾Ø¯Ø§Ø± Ù„Ú‘Ú©ÛŒ ÛØ§ Ù…Ø±ÛŒÙ… ...\n",
      "31                                      Ø¢Ù…ÛŒÙ† Ø«Ù…Û Ø¢Ù…ÛŒÙ†\n",
      "32  Ø¬ÛŒØªÚ¾Û Ú©Ú¾ÙˆØªÛŒ ÙˆØªÚ¾Û Ø¢ Ú©Ú¾Ù„ÙˆØªÛŒ positivepositiveposi...\n"
     ]
    }
   ],
   "source": [
    "data_frame['urdu_text'] = data_frame['stemmed_text']\n",
    "print(\"Final Preprocessed Urdu Text:\")\n",
    "print(data_frame[['urdu_text']].head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b17681",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9cb2377a-15e9-42af-834e-173a3c2d2eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Data After Lemmatization:\n",
      "                                        stemmed_text  \\\n",
      "0  positivepositivepositive Ù„ÛŒÙ†Û Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ...   \n",
      "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†Ø§ Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†Ø§ Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚºpo...   \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾ÙˆØ²ÛŒØ´Ù†...   \n",
      "4      Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø¢Ø¦ÛŒ Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±positive   \n",
      "5             Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø± negativepositive   \n",
      "\n",
      "                                     lemmatized_text  \n",
      "0  positivepositivepositive Ù„ÛŒÙ†Û Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ...  \n",
      "1  Ú†Ù„Ù†Ø§ Ù…ÛÙ…Ø§Ù†Ø§ Ú©Ú¾Ø§ Ø³Ø±Ø§ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†Ø§ Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚºpo...  \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ù†Ø§ Ø¬Ø§Ù†Ø§ Ø§Ù¾ÙˆØ²ÛŒØ´Ù†...  \n",
      "4      Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø¢Ø¦ÛŒ Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±positive  \n",
      "5             Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø± negativepositive  \n",
      "Final Preprocessed Urdu Text:\n",
      "                                            urdu_text\n",
      "0   positivepositivepositive Ù„ÛŒÙ†Û Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ...\n",
      "1   Ú†Ù„Ù†Ø§ Ù…ÛÙ…Ø§Ù†Ø§ Ú©Ú¾Ø§ Ø³Ø±Ø§ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†Ø§ Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚºpo...\n",
      "2   Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ù†Ø§ Ø¬Ø§Ù†Ø§ Ø§Ù¾ÙˆØ²ÛŒØ´Ù†...\n",
      "4       Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø¢Ø¦ÛŒ Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±positive\n",
      "5              Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø± negativepositive\n",
      "6                               Ø§Ù†Ø³Ø§Úº ØªÚ¾Ú©Ù†Ø§ Ø³ÙˆÚ†Ù†Ø§ Ø³ÙØ±\n",
      "7                              Ø­Ø§Ù…Ø¯ Ù…ÛŒØ± ÙˆÛŒÙ„ÚˆÙ†positive\n",
      "8   ÛŒØ§Ø± ÙˆÚ†Ø§Ø±Û ÙˆÛŒÙ„Ø§ ÛÙˆÙ†Ø¯Ø§ Ø¢Ø±Û ÛÙˆÛŒØ§ ÛÛ’positivepositi...\n",
      "9   Ø³Ù…Ø¬Ú¾ØªÛ Ø³Ø§Ø±Ø§ Ù¾Ø§Ú©Ø³ØªÙ† Ø¨ÛŒÙˆÙ‚ÙˆÙ Ú¾Û positivepositivep...\n",
      "10    ØªØ³ÛŒ Ù„Ú‘Ø§ÚºÙ”ÛŒ Ú©Ø±ÙˆØ§Ù†ÛŒ Ø³Ø§ÚˆÛŒ positivepositivepositive\n",
      "11                          Ù¾Ø§Ø¦Ù† Ø¯ÙˆØ¨Ø±Û ÙØ§Ù„Ùˆ Ú©Ø±Ø¦ÛŒÛ ØŸ ØŸ\n",
      "12  Ú©ØªÙ†Ø§ Ù…ÛÙ†Ú¯Ø§Ø¦ÛŒ Ø§Ù„Û Ø¯ÙˆØ³Ùˆ Ø±ÙˆÙ¾Û Ø¯Ø±Ø¬Ù† Ú©Ø¯Û 80Ø±ÙˆÙ¾Û Ú¯Ø² ...\n",
      "13  Ø¹Ø´Ù‚ Ø±Ø§Ø³ Ø¢Û“ negativeØ²Ø®Ù… Ú©Ú¾Ø§Ù¶ Ú¯Û positiveÙ…ÙØ³Ú©Ø±Ø§Ù¶ Ú¯Û\n",
      "15  Ø®Ø§ØªÙ… Ø§Ù„Ù†Ø¨ÛŒÛŒÙ† Ù…Ø­Ù…Ø¯ï·º Surat 73 Ø³ÙˆØ±Ù‡ Ø§Ù„Ù…Ø²Ù…Ù„ Ayt 20...\n",
      "16                Ø¨ÛŒÚ†Ø§Ø±Û Ø¨ÛŒÙˆÛŒ ÛÛŒÛ ØªÛŒØ³Ø±ÛŒ Ù¹Ø±Ø§Ø¦ positive\n",
      "17              Ù¾ØªÛ ÛÙˆØ±ÛØ§ Ø¨ÙˆØ±Úˆ Ø²Ù†Ú¯ ÛÛ’negativenegative\n",
      "18                 Ø§Ù„Ù„Û Ø±Ø­Ù…Øª Ø³Ø§Ø¦Û Ù…Ú©Ù…Ù„ ØµØ­Øª ÛŒØ§Ø¨ ÙØ±Ù…Ø§Ø¦Û\n",
      "19           Ø¢Ø±Ù…ÛŒ Ú†ÛŒÙ Ù†ÙˆÙ¹Ø³ Û” Ù…Ø¬Ø±Ù… Ø¬Ø±Ø§Ø¦ÛŒÙ… Ù†ÙˆÙ¹Ø³ Ú©Ø±Û’Ú¯Ø§ Û”\n",
      "20  Ú©Ú¾ÙˆÙ†Ø§ Ø¨Ú†ÛŒØ§ØŒØªÛŒØ±ÛŒ Ø¹Ø²Øª Ø§ÛØŒØªÛŒØ±ÛŒ Ù‚Ø¯Ø± Ø§Û Û” Ø´Ú©Ø± Ø¨ÙˆÙ†Úˆ ...\n",
      "21  ÙÙˆØ¬ Ø³ÛŒØ§Ø³Øª Ø¯ÙˆØ± Û” Ø¨Ù„Ø§ÙˆÙ„ Ø²Ø±Ø¯Ø§Ø±ÛŒ ÙÙˆØ¬ Ú©Ø±Ø§Ú†ÛŒ ÙˆØ§Ù‚Ø¹Û Ù†...\n",
      "22                                 Ù„Ú‘Ú©ÛŒ Ø¬ÛŒÙ†Ø§ positive\n",
      "23  ØµØ¯Ø§Ø¦Ø§ Ø¯Ø±ÙˆØ¯Ø§ Ø±ÛÛŒÙ†Ú¯ÛŒ Ø³Ù†Ù†Ø§ Ø¯Ù„ Ø´Ø§Ø¯ Ú¯Ø§ØŒ Ø®Ø¯Ø§ Ø§ÛÙ„Ø³Ù†Øª ...\n",
      "24                              ÙˆÙˆÙ¹ Ø¨Ø¬Ø§Ø¦Û ÙˆÙˆÙ¹Ø± Ø¹Ø²Ù‘Øª ï¸\n",
      "25  2004 Ù†Ø´Ø¦ÛŒ Ú©ÛÙ†Ø§ Ø§Ø³Ù¹ÛŒØ¨Ù„Ø´Ù…Ù†Ù¹ Ú©Øª ÙˆØ²ÛŒØ±Ø§Ø¹Ø¸Ù… 2018 Ú©ØªÛ...\n",
      "26                      Ù…Ø´Ú©Ù„ Ù†Ø§Ù…Ù…Ú©Ù† Ú©Ø§Ù…Ø±Ø§Ù† Ú©ÙˆØ´Ø´ Û” Û” ØŸ\n",
      "27        Ù¾ÙˆØ±Û ÛÙØªÛ Ø³Ø§Ù„Ù† Ø±ÙˆÙ¹ÛŒ Ú©Ú¾Ø§Ù†Ø§ Ú©ÙˆØ¦ÛŒ Ø­Ø§Ù„ negative\n",
      "29  ÙˆÚ¾ Ú©ÙˆØ¦ÛŒ Ú¾Ùˆ Ø¨ÛŒØ´Ú© Ú¾Û Ú¾Ùˆ Ú©ÛŒÙ…Ù†Ø§ Ø­Ø±Ø§Ù… Ú¾Ùˆ Ø­Ù‚ Ø¨Ù‡ Ú©Ø§Ù…Ø±...\n",
      "30  Ù…Ø§Ø¶ÛŒ Ø§Ù†ØªÛØ§Ø¦ÛŒ Ø¨Û ÙˆÙ‚ÙˆÙ Ø­Ø§Ù„ Ø³Ù…Ø¬Ú¾Ø¯Ø§Ø± Ù„Ú‘Ú©ÛŒ ÛØ§ Ù…Ø±ÛŒÙ… ...\n",
      "31                                      Ø¢Ù…ÛŒÙ† Ø«Ù…Û Ø¢Ù…ÛŒÙ†\n",
      "32  Ø¬ÛŒØªÚ¾Û Ú©Ú¾ÙˆÙ†Ø§ ÙˆØªÚ¾Û Ø¢Ù†Ø§ Ú©Ú¾Ù„ÙˆØªÛŒ positivepositivepo...\n"
     ]
    }
   ],
   "source": [
    "def apply_lemmatization(text):\n",
    "    lemmatized_text = textProcessor.lemmatize_sentence(text)\n",
    "    return lemmatized_text\n",
    "\n",
    "# Apply lemmatization to the 'stemmed_text' column\n",
    "data_frame['lemmatized_text'] = data_frame['stemmed_text'].apply(lambda x: apply_lemmatization(str(x)) if pd.notnull(x) else x)\n",
    "\n",
    "# Display the first few rows after lemmatization\n",
    "print(\"\\nSample Data After Lemmatization:\")\n",
    "print(data_frame[['stemmed_text', 'lemmatized_text']].head())\n",
    "\n",
    "data_frame['urdu_text'] = data_frame['lemmatized_text']\n",
    "print(\"Final Preprocessed Urdu Text:\")\n",
    "print(data_frame[['urdu_text']].head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a77d91-93f9-4236-ae87-acbbd24f3e3c",
   "metadata": {},
   "source": [
    "# Phase 3: Feature Extraction from Urdu Text using LughaatNLP and Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5faaae16-aacc-4df3-bef6-0f7980ea5961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eaf75b-a66f-4f50-8019-6c4ea00befec",
   "metadata": {},
   "source": [
    "# Step 1: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b4a0d2e1-f9dc-4e3c-ac16-9aa886280db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Data After Tokenization:\n",
      "                                           urdu_text  \\\n",
      "0  positivepositivepositive Ù„ÛŒÙ†Û Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ...   \n",
      "1  Ú†Ù„Ù†Ø§ Ù…ÛÙ…Ø§Ù†Ø§ Ú©Ú¾Ø§ Ø³Ø±Ø§ Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†Ø§ Ø¯Ø³Ø¯ÛŒ Ø¢Úº Ù…ÛŒÚºpo...   \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ù†Ø§ Ø¬Ø§Ù†Ø§ Ø§Ù¾ÙˆØ²ÛŒØ´Ù†...   \n",
      "4      Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¢Ø¦ÛŒ Ø¢Ø¦ÛŒ Ø­Ø§Ù…Ø¯ Ù…ÛŒØ±positive   \n",
      "5             Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø± negativepositive   \n",
      "\n",
      "                                      tokenized_text  \n",
      "0                    [Ù„ÛŒÙ†Û, Ø´Ø§Ø¯ÛŒ, ÙØ³Ø§Ø¯Ù†, Ù¹Ú¾ÛŒÚ©, Ú©ÙˆØ¬ÛŒ]  \n",
      "1  [Ú†Ù„Ù†Ø§, Ù…ÛÙ…Ø§Ù†Ø§, Ú©Ú¾Ø§, Ø³Ø±Ø§, Ú†Ú‘ÛŒÙ„, Ú†Ø§Ú†ÛŒ, Ù†Ø§, Ø¯Ø³Ø¯ÛŒ,...  \n",
      "2  [Ú©Ø§Ù…Ø±Ø§Ù†, Ø®Ø§Ù†, Ø¯Ù†, Ø¨Ú¾Ø±ÛŒÛ, Ø²Ù…Û, Ø¯Ø§Ø±ÛŒ, Ù„Ú¯Ù†Ø§, Ø¬Ø§Ù†Ø§...  \n",
      "4    [Ù…Ø±Ø§Ø¯, Ø¹Ù„ÛŒ, Ø´Ø§Û, Ø¨Ú¾ÛŒØ³, ÚˆÛŒ, Ø¢Ø¦ÛŒ, Ø¢Ø¦ÛŒ, Ø­Ø§Ù…Ø¯, Ù…ÛŒØ±]  \n",
      "5                         [Ù‚Ø§Ø¨Ù„, Ø§Ø¹ØªØ¨Ø±, Ù‚Ø§ØªÙ„, Ø§Ø¹ØªØ¨Ø±]  \n"
     ]
    }
   ],
   "source": [
    "# Function to tokenize the text using LughaatNLP's urdu_tokenize function\n",
    "def tokenize_text(text):\n",
    "    tokens = textProcessor.urdu_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization to the 'lemmatized_text' column\n",
    "data_frame['tokenized_text'] = data_frame['lemmatized_text'].apply(lambda x: tokenize_text(str(x)) if pd.notnull(x) else x)\n",
    "\n",
    "# Display the first few rows after tokenization\n",
    "print(\"\\nSample Data After Tokenization:\")\n",
    "print(data_frame[['urdu_text', 'tokenized_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3e5722-195e-4fc2-8359-f49b103ef465",
   "metadata": {},
   "source": [
    "# Step 2: TF-IDF (Term Frequency-Inverse Document Frequency) Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3a137e7c-6c53-4c90-833c-71d4a69ce425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Words with Highest TF-IDF Scores:\n",
      "Û”                           0.133195\n",
      "positive                    0.080712\n",
      "positivepositive            0.035554\n",
      "positivepositivepositive    0.028631\n",
      "ØŸ                           0.028484\n",
      "Ø§Ù„Ù„Û                        0.027203\n",
      "Ú©ÙˆØ¦ÛŒ                        0.027128\n",
      "Ø®Ø§Ù†                         0.025684\n",
      "Ø¨Ù‡                          0.023354\n",
      "Ú¾Û                          0.022596\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the TF-IDF Vectorizer for Urdu text\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(), max_features=100)\n",
    "\n",
    "# Fit and transform the lemmatized text to extract TF-IDF features\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data_frame['urdu_text'])\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame for better readability\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the top 10 words with the highest average TF-IDF scores\n",
    "top_tfidf_words = tfidf_df.mean().sort_values(ascending=False).head(10)\n",
    "print(\"\\nTop 10 Words with Highest TF-IDF Scores:\")\n",
    "print(top_tfidf_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cc4fbc-3610-42f0-85f7-c63d498e2650",
   "metadata": {},
   "source": [
    "# Step 3: Word2Vec Model Training and Similarity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2830924e-c503-4799-ac33-c4aa6ba1a9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 Words Most Similar to 'Ø§Ú†Ú¾Ø§':\n",
      "Ø¨Ú†Û: 0.9874\n",
      "Ø­Ø§Ù„: 0.9860\n",
      "Ø¢Ø¦Û: 0.9840\n",
      "Ø³ÙˆÚ†: 0.9822\n",
      "Ø¨Øª: 0.9810\n"
     ]
    }
   ],
   "source": [
    "# Prepare the tokenized text for Word2Vec training\n",
    "tokenized_texts =data_frame['tokenized_text'].tolist()\n",
    "\n",
    "# Train the Word2Vec model using the tokenized text\n",
    "word2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=8, min_count=2, workers=4)\n",
    "\n",
    "# List the top 5 words most similar to the word \"Ø§Ú†Ú¾Ø§\" (good)\n",
    "try:\n",
    "    similar_words = word2vec_model.wv.most_similar(\"Ø§Ú†Ú¾Ø§\", topn=5)\n",
    "    print(\"\\nTop 5 Words Most Similar to 'Ø§Ú†Ú¾Ø§':\")\n",
    "    for word, similarity in similar_words:\n",
    "        print(f\"{word}: {similarity:.4f}\")\n",
    "except KeyError:\n",
    "    print(\"\\nThe word 'Ø§Ú†Ú¾Ø§' is not in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12359fe8-306c-4292-b5c9-7d0c1ea77c2c",
   "metadata": {},
   "source": [
    "# Phase 4: N-grams Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "49cb5041-7368-4d2b-8b33-332b7181ff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "tokenized_texts = data_frame['tokenized_text'].tolist()  \n",
    "\n",
    "# Flatten the tokenized texts for unigram analysis\n",
    "all_words = [word for tokens in tokenized_texts for word in tokens]\n",
    "\n",
    "# Unigrams\n",
    "unigram_freq = Counter(all_words)\n",
    "\n",
    "# Bigrams\n",
    "bigram_list = [list(ngrams(tokens, 2)) for tokens in tokenized_texts]\n",
    "bigrams = [bigram for sublist in bigram_list for bigram in sublist]  # Flatten\n",
    "bigram_freq = Counter(bigrams)\n",
    "\n",
    "# Trigrams\n",
    "trigram_list = [list(ngrams(tokens, 3)) for tokens in tokenized_texts]\n",
    "trigrams = [trigram for sublist in trigram_list for trigram in sublist]  # Flatten\n",
    "trigram_freq = Counter(trigrams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f61dd5",
   "metadata": {},
   "source": [
    "# Get the top 10 most common unigrams, bigrams, and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ecda4038-8c72-4605-8510-423645783841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Unigrams:\n",
      "Û”: 11547\n",
      "ØŸ: 1580\n",
      "ï¸: 1560\n",
      "Ú©ÙˆØ¦ÛŒ: 1174\n",
      "Ø®Ø§Ù†: 1172\n",
      "Ú¾Û: 1067\n",
      "Ø§Ù„Ù„Û: 989\n",
      "Ø¨Ù‡: 976\n",
      "Ù¾Ø§Ú©Ø³ØªÙ†: 859\n",
      "Ø³Ù†Ø¯Ú¾: 815\n",
      "\n",
      "Top 10 Bigrams:\n",
      "('Û”', 'Û”'): 4685\n",
      "('Ø¹Ù…Ø±Ø§Ù†', 'Ø®Ø§Ù†'): 508\n",
      "('ØŸ', 'ØŸ'): 496\n",
      "('Ù†ÙˆØ§Ø²', 'Ø´Ø±ÛŒÙ'): 448\n",
      "('\\u200d', 'ï¸'): 332\n",
      "('Ø³Ù†Ø¯Ú¾', 'Ù¾ÙˆÙ„ÛŒØ³'): 307\n",
      "('ï¸', 'ï¸'): 260\n",
      "('Ø¢Ø±Ù…ÛŒ', 'Ú†ÛŒÙ'): 223\n",
      "('Ú©ÛŒÙ¾Ù¹Ù†', 'ØµÙØ¯Ø±'): 179\n",
      "('Ø§Ø±Ø¯Ùˆ', 'Ø²Ø¨Ù†'): 172\n",
      "\n",
      "Top 10 Trigrams:\n",
      "('Û”', 'Û”', 'Û”'): 2302\n",
      "('ØŸ', 'ØŸ', 'ØŸ'): 234\n",
      "('\\u2066', 'ï¸', '\\u2069'): 131\n",
      "('ï¸', 'ï¸', 'ï¸'): 121\n",
      "('\\u200d', 'ï¸', '\\u200d'): 117\n",
      "('ï¸', '\\u200d', 'ï¸'): 116\n",
      "('Ù¾ÛŒÙ†Ø§', 'Ù¹ÛŒ', 'Ø¢Ø¦ÛŒ'): 114\n",
      "('Ù¾ÛŒÙ†Ø§', 'ÚˆÛŒ', 'Ø§ÛŒÙ…'): 88\n",
      "('ØµÙ„ÛŒ', 'Ø§Ù„Ù„Û', 'Ø¹Ù„ÛŒÛ'): 88\n",
      "('Ø¬Ø²Ø§Ú©', 'Ø§Ù„Ù„Û', 'Ø®ÛŒØ±'): 74\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "top_unigrams = unigram_freq.most_common(10)\n",
    "top_bigrams = bigram_freq.most_common(10)\n",
    "top_trigrams = trigram_freq.most_common(10)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nTop 10 Unigrams:\")\n",
    "for word, freq in top_unigrams:\n",
    "    print(f\"{word}: {freq}\")\n",
    "\n",
    "print(\"\\nTop 10 Bigrams:\")\n",
    "for bigram, freq in top_bigrams:\n",
    "    print(f\"{bigram}: {freq}\")\n",
    "\n",
    "print(\"\\nTop 10 Trigrams:\")\n",
    "for trigram, freq in top_trigrams:\n",
    "    print(f\"{trigram}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0c643e-53cd-48ad-85fe-cf644f73e182",
   "metadata": {},
   "source": [
    "# Phase 5: Sentiment Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e783e0b-92ac-419c-b0df-bb0d1bf72529",
   "metadata": {},
   "source": [
    "# tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "75d09847-72c6-45e2-98b8-117c58448f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7126\n",
      "Precision: 0.7428\n",
      "Recall: 0.7095\n",
      "F1 Score: 0.7258\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming 'labels' column has the sentiment labels (1 for positive, 0 for negative/neutral)\n",
    "X = tfidf_matrix\n",
    "y = data_frame['is_sarcastic']  # replace with your actual label column\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba2cfba-89dd-4dc6-b4c6-e6d9f35b8cbf",
   "metadata": {},
   "source": [
    "Evaluation Summary\n",
    "Accuracy: 71.26%\n",
    "The model correctly classified approximately 71 out of 100 instances in the validation set.\n",
    "Precision: 74.28%\n",
    "Of all the positive predictions made by the model, 74.28% were accurate, indicating a reduced number of false positives compared to previous models.\n",
    "Recall: 70.95%\n",
    "The model identified 70.95% of the actual positive sentiments, showing it maintains good coverage of relevant instances.\n",
    "F1 Score: 72.58%\n",
    "This score reflects a balance between precision and recall, indicating an overall improvement in performance compared to earlier results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80327b54-78a4-49a8-b37b-56252690deb7",
   "metadata": {},
   "source": [
    "# 1. Evaluation\n",
    "#Model Performance\n",
    "The sentiment analysis model achieved an accuracy of 71.26%, with precision at 74.28%, recall at 70.95%, and an F1 score of 72.58%. These metrics indicate that the model is fairly balanced, with a good ability to identify positive sentiments but showing some difficulty with negative sentiments. For instance, it may misinterpret nuanced phrases like â€œÛŒÛ Ø¨ÛØª Ø¨Ø±Ø§ ÛÛ’â€ (This is very bad) if it fails to grasp the context or the negation, leading to inaccuracies in sentiment classification.\n",
    "# Challenges with Stemming and Lemmatization\n",
    "Stemming and lemmatization are particularly challenging in Urdu due to the languageâ€™s complex morphology. Words can change forms based on gender, plurality, or tense. For example, the word \"Ø§Ú†Ú¾Ø§\" (good) can appear as \"Ø§Ú†Ú¾ÛŒ\" (good, feminine) and \"Ø§Ú†Ú¾Û’\" (good, plural). This variation can confuse the model, especially in contexts like â€œÛŒÛ Ø§Ú†Ú¾Ø§ Ù†ÛÛŒÚº ÛÛ’â€ (This is not good), where understanding negation is crucial for accurate sentiment analysis. If the model does not recognize these morphological changes, it may misclassify the overall sentiment.\n",
    "# Areas for Improvement\n",
    "To enhance performance, we should focus on refining preprocessing techniques, such as applying more advanced stemming algorithms that accommodate the richness of Urdu. Implementing methods like analysis and utilizing pre-trained models that consider context could significantly improve sentiment classification accuracy, especially for complex sentence structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a4c6d0-6ee0-42be-933e-937ef669c5e0",
   "metadata": {},
   "source": [
    "# 2. Challenges in Urdu Sentiment Analysis\n",
    "# Complex Morphology\n",
    "Urduâ€™s intricate word forms pose significant challenges for stemming and lemmatization. For instance, the root â€œÚ†Ù„Ù†Ø§â€ (to walk) can manifest as â€œÚ†Ù„ØªØ§â€ (masculine), â€œÚ†Ù„ØªÛŒâ€ (feminine), and â€œÚ†Ù„ÛŒÚº Ú¯Û’â€ (will walk, plural). This complexity can lead to context loss during analysis, misclassifying sentiments in sentences such as â€œÙˆÛ ÛÙ…ÛŒØ´Û Ú†Ù„ØªÛŒ Ø±ÛØªÛŒ ÛÛ’â€ (She keeps walking always), where the sentiment may hinge on understanding the specific word forms used.\n",
    "# Colloquial Language\n",
    "The informal and colloquial nature of language used on social media introduces slang and abbreviations that complicate sentiment analysis. Phrases like â€œØ¨ÛØª Ø²Ø¨Ø±Ø¯Ø³Øªâ€ (very awesome) or â€œÚ©Ù…Ø§Ù„ ÛÛ’â€ (it's amazing) might not align with formal language models. Additionally, expressions such as â€œÙ†ÛÛŒÚº ÛŒØ§Ø±â€ (no man) can convey sarcasm but may be misinterpreted by the model as literal negation, leading to incorrect sentiment classification.\n",
    "# Noisy Data\n",
    "Social media platforms are filled with noisy data, including emojis, misspellings, and unconventional grammar. For example, a post containing â€œÛŒÛ ØªÙˆ Ø¨ÛØª ğŸ˜‚ ÛÛ’â€ (This is so funny) may confuse the model due to the emotional context of the emoji. Typos, such as â€œØ¨Ø±Ø§â€ (bad) being written as â€œØ¨Ø±Ø§Ûâ€ or â€œØ¨Ú‘Ø§â€ (big), can further hinder the model's ability to extract clear sentiment signals.\n",
    "# Optimizing the NLP Pipeline\n",
    "To enhance Urdu sentiment analysis, we should implement better data normalization techniques, including spell-checking algorithms and expanding the training dataset to encompass a wider variety of colloquial expressions. Utilizing advanced language models, like transformers specifically designed for Urdu, can also capture contextual meanings more effectively. By addressing these challenges and adopting these strategies, we can significantly improve the accuracy and robustness of sentiment analysis in this rich language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fe7684-22b2-421b-93c5-4dd276574936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
